<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Manifold Hypothesis | Ravjoth Brar</title>
    <meta name="description" content="Understanding the Manifold Hypothesis - the elegant principle driving deep learning's success in pattern recognition.">
    <link rel="canonical" href="https://ravjothbrar.com/blog/manifold-hypothesis.html" />
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <!-- Corner Decorations -->
    <div class="corner corner-tl"></div>
    <div class="corner corner-tr"></div>
    <div class="corner corner-bl"></div>
    <div class="corner corner-br"></div>

    <!-- Side Navigation -->
    <nav class="side-nav">
        <ul>
            <li><a href="../index.html#home" class="nav-link">> Home</a></li>
            <li><a href="../index.html#about" class="nav-link">> About Me</a></li>
            <li><a href="../index.html#links" class="nav-link">> Links</a></li>
            <li><a href="../index.html#roles" class="nav-link">> Roles</a></li>
            <li><a href="../index.html#projects" class="nav-link">> Projects</a></li>
            <li><a href="../blog.html" class="nav-link active">> Blog</a></li>
            <li><a href="../index.html#publications" class="nav-link">> Publications</a></li>
            <li><a href="../index.html#cv" class="nav-link">> CV</a></li>
            <li><a href="../index.html#contact" class="nav-link">> Contact Me</a></li>
        </ul>
    </nav>

    <!-- Main Content -->
    <main class="main-content blog-article-page">
        <!-- Article Header -->
        <article class="section">
            <div class="article-header">
                <div class="article-meta">
                    <a href="../blog.html" class="back-link">&larr; Back to Blog</a>
                    <span class="article-date">10/02/2025</span>
                </div>
                <h1 class="article-title">> The Manifold Hypothesis: The Elegant Principle Driving Deep Learning's Success</h1>
            </div>

            <!-- Cover Image -->
            <div class="article-cover">
                <img src="images/manifold-hypothesis.png" alt="Manifold Hypothesis Cover" class="article-cover-image">
            </div>

            <div class="terminal-box article-content">
                <p>Have you ever wondered why we can so effortlessly spot a familiar face in a crowd?</p>

                <p>Despite the seemingly infinite variations of faces around us, your brain employs an astonishing principle known as the 'Manifold Hypothesis'.</p>

                <p>In this article we will:</p>

                <ul>
                    <li>Dive into the fascinating paradox surrounding Deep Learning</li>
                    <li>Grapple with the underlying concepts of Manifolds</li>
                    <li>Explain how modern neural networks utilise the Manifold Hypothesis</li>
                    <li>Discuss the possibility of whether Manifolds could lead us to AGI</li>
                </ul>

                <h2>Deep Learning Shouldn't Work</h2>

                <p>At its core, deep learning is inherently flawed; it shouldn't make sense.</p>

                <p>Consider this: a 1000x1000 pixel color image, where each pixel has three color values (RGB), results in a staggering 3 million dimensions to examine. The number of possible images you can make within these dimensions is astronomical — even larger than the number of atoms in the known universe.</p>

                <p>Yet somehow neural networks can learn to identify patterns from relatively small datasets.</p>

                <p>How is this possible?</p>

                <p>The answer, as always in AI, lies in the data itself. Despite the incredibly high-dimensional space (3 million), the data itself resides within a lower-dimensional structure known as a manifold.</p>

                <h2>Understanding Manifolds: A Journey Through Dimensions</h2>

                <p>The key insight behind manifolds: they can twist through higher-dimensional spaces while maintaining a simpler internal structure.</p>

                <p>But what does this actually mean?</p>

                <p>This means, taking our image example, there is a simple internal structure that governs the images. This then twists to reach such high dimensional spaces.</p>

                <p>Now, let's apply this to faces. A face of a person might have millions of pixels, but it doesn't have millions of dimensions. Instead, faces have a few dimensions of what they can do:</p>

                <ul>
                    <li>Head rotation (left/right & up/down)</li>
                    <li>Expression (happy, sad, angry, surprised)</li>
                    <li>Lighting conditions</li>
                    <li>Age</li>
                    <li>Other Identity characteristics</li>
                </ul>

                <p>Whilst this break-down may seem like a simplistic representation, the same principle applies to all natural data including written text, and even how you talk!</p>

                <h2>The Dimensional Dance</h2>

                <p>Neural networks use the Manifold Hypothesis as a set structure for how to generate images/text.</p>

                <p>Each layer in a neural network correlates to the lower dimensions, thus the more layers, the more dimensions a neural network can tackle.</p>

                <p>Returning to our image example, a deep learning algorithm may follow these steps:</p>

                <ul>
                    <li><strong>Initial Layers:</strong> Identify and map nearby relationships between pixels</li>
                    <li><strong>Intermediate layers:</strong> Untwist the high dimensional space to low dimensions, similar to smoothening out a crumpled sheet of paper</li>
                    <li><strong>Deeper layers:</strong> Group similar pixels together and separate them from others</li>
                </ul>

                <p>These gradual steps outline how deep learning algorithms unfold high-dimensional images into a manifold. Larger neural networks excel because they can navigate these layers with remarkable efficiency.</p>

                <h2>Manifolds: The Missing Link to AGI</h2>

                <p>So when you recognise that familiar face, your brain isn't actually processing millions of pixels but rather following the many intricate pathways encoded in our brain.</p>

                <p>These pathways are nature's own manifolds in action, and they appear in everything we do!</p>

                <p>Your brain processes roughly 11 million bits of information every single second, but you are only consciously aware of about 50.</p>

                <p>The human brain is so remarkable that the key to AGI is us.</p>

                <p>Finding a way to emulate the human brain is our best bet as achieving AGI — not through brute force computation, but by following nature's elegant blueprint.</p>

                <p>And I for one, can't wait to see where it leads.</p>
            </div>
        </article>

        <!-- Footer -->
        <footer class="footer">
            <p>&copy; 2026 Ravjoth Brar. All rights reserved. <span class="footer-note">Kindly Hosted on GitHub Pages</span></p>
        </footer>
    </main>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const terminalBox = document.querySelector('.terminal-box');
            if (terminalBox) {
                terminalBox.style.opacity = '0';
                terminalBox.style.transform = 'translateY(20px)';
                terminalBox.style.transition = 'opacity 0.8s cubic-bezier(0.16, 1, 0.3, 1), transform 0.8s cubic-bezier(0.16, 1, 0.3, 1)';

                setTimeout(() => {
                    terminalBox.style.opacity = '1';
                    terminalBox.style.transform = 'translateY(0)';
                }, 100);
            }
        });
    </script>
</body>
</html>
